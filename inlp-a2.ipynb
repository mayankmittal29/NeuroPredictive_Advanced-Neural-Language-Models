{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-02-17T05:40:58.644681Z","iopub.status.busy":"2025-02-17T05:40:58.644473Z","iopub.status.idle":"2025-02-17T05:40:59.646182Z","shell.execute_reply":"2025-02-17T05:40:59.645270Z","shell.execute_reply.started":"2025-02-17T05:40:58.644659Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/corpus/Ulysses.txt\n","/kaggle/input/corpus/Pride_and_Prejudice.txt\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2025-02-17T05:42:28.144539Z","iopub.status.busy":"2025-02-17T05:42:28.144246Z","iopub.status.idle":"2025-02-17T05:42:30.588623Z","shell.execute_reply":"2025-02-17T05:42:30.587861Z","shell.execute_reply.started":"2025-02-17T05:42:28.144517Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Welcome to the Toknizer:\n","Thank you for using tokenizer bye!\n"]}],"source":["import re\n","import string\n","import sys\n","\n","class Tokenizer:\n","    def __init__(self, corpus):\n","        self.corpus = corpus\n","        self.preprocess_corpus()\n","        self.sen_tokenize = re.compile(r'(?<=[.!?]\")[\\s]|(?<=[.!?])[\\s]')\n","        self.word_and_punct_tokenize = re.compile(r'''(?:[A-Z]\\.)+|\\w+(?:-\\w+)*|\\w+(?:'\\w+)?|\\.\\.\\.|(?:Mr|Mrs|Dr|Ms)\\.|\\w+|[^\\w\\s]|'\\w+''')\n","        self.num_tokenize = re.compile(r'\\b\\d+(\\.\\d+)?\\b')\n","        self.mail_tokenize = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n","        self.url_tokenize = re.compile(r'(http[s]?://|www\\.)(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n","        self.hash_tokenize = re.compile(r'#[\\w\\-]+')\n","        self.mention_tokenize = re.compile(r'@[\\w\\-]+')\n","        self.money_tokenize = re.compile(r'\\b\\d+(\\.\\d+)?\\s?\\$|\\$\\s?\\d+(\\.\\d+)?\\b')\n","        self.percent_tokenize = re.compile(r'\\b\\d+(\\.\\d+)?\\%')\n","        self.age_tokenize = re.compile(r'\\b\\d{1,3}(?:\\s|-)?(?:year(?:s)?\\s?-?\\s?old)\\b')\n","        self.time_tokenize = re.compile(r'\\b\\d{1,2}:\\d{2}\\s?(?:AM|PM)?\\b|\\b(?:morning|afternoon|evening|night)\\b', flags=re.IGNORECASE)\n","        self.date_tokenize = re.compile(r'\\b(?:\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}|\\d{2,4}[-/]\\d{1,2}[-/]\\d{1,2}|\\d{1,2}\\s(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s\\d{2,4}|\\d{1,2}\\s(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{2,4})\\b')\n","\n","    def preprocess_corpus(self):\n","        self.corpus = self.corpus.lower()\n","        self.corpus = re.sub(r'\\s+', ' ', self.corpus)\n","        alphanumeric_tokenize = re.compile(r'(\\d+|\\D+)')\n","        self.corpus = ' '.join([' '.join(alphanumeric_tokenize.findall(word)) if alphanumeric_tokenize.match(word) and '.' not in word else word for word in self.corpus.split()])\n","        self.corpus = self.corpus.replace(\"_\", \"\")\n","        self.corpus = re.sub(r'\\b(mr|mrs|ms|dr|prof|rev|rd|st|gen|rep|sen|sr|jr)\\.', r'\\1', self.corpus)\n","\n","    def replace_entities(self, text):\n","        text = re.sub(self.url_tokenize, '<URL>', text)\n","        text = re.sub(self.mail_tokenize, '<MAILID>', text)\n","        text = re.sub(self.date_tokenize, '<DATE>', text)\n","        text = re.sub(self.time_tokenize, '<TIME>', text)\n","        text = re.sub(self.age_tokenize, '<AGE>', text)\n","        text = re.sub(self.percent_tokenize, '<PERCENTAGE>', text)\n","        text = re.sub(self.money_tokenize, '<CURRENCY>', text)\n","        text = re.sub(self.num_tokenize, '<NUM>', text)\n","        text = re.sub(self.mention_tokenize, '<MENTION>', text)\n","        text = re.sub(self.hash_tokenize, '<HASHTAG>', text)\n","        return text\n","    \n","    def tokenize_sentence(self, text):\n","        sentences = re.split(self.sen_tokenize, text)\n","        sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n","        return sentences\n","\n","    def tokenize_word(self, sentence):\n","        words_and_punctuations = []\n","        \n","        # Keep special tokens and words, but remove standalone punctuations\n","        tokens = re.findall(r'<URL>|<MAILID>|<DATE>|<TIME>|<AGE>|<PERCENTAGE>|<CURRENCY>|<NUM>|<MENTION>|<HASHTAG>|' + self.word_and_punct_tokenize.pattern, sentence)\n","        \n","        for token in tokens:\n","            # Remove pure punctuation tokens (excluding predefined tags)\n","            if token.isalnum() or token.startswith(\"<\") and token.endswith(\">\"):\n","                words_and_punctuations.append(token)\n","        \n","        return words_and_punctuations\n","\n","    def tokenize(self):\n","        self.corpus = self.replace_entities(self.corpus)\n","        sentences = self.tokenize_sentence(self.corpus)\n","        tokenized_corpus = [self.tokenize_word(sentence) for sentence in sentences]\n","        return tokenized_corpus\n","\n","def tokenize_sentences(text):\n","    # sentences = re.split(\n","    #     r'(?<=[.!?])\"? | (?<=[.!?])\\s+(?=[A-Z])', text.strip())\n","\n","    patterns = {\n","        \"percentage\": r\"\\b\\d+(\\.\\d+)?%\",  # Match percentages like 45%\n","        \"url\": r\"(https?://\\S+|www\\.\\S+)\",  # Match URLs\n","        # Match emails\n","        \"email\": r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\",\n","        \"hashtag\": r\"#\\w+\",  # Match hashtags\n","        \"mention\": r\"@\\w+\",  # Match mentions\n","        \"age\": r\"\\b\\d{1,3}\\s?(years old|yo|yrs)\\b\",  # Match age patterns\n","        # Match times like 12:30 PM\n","        \"time\": r\"\\b\\d{1,2}:\\d{2} ?(AM|PM|am|pm)?\\b\",\n","        \"number\": r\"\\b\\d+(\\.\\d+)?\\b\",  # Match any number\n","        \"date\": r\"\\b\\d{1,2}(?:st|nd|rd|th)?\\s+(?:January|February|March|April|May|June|July|August|September|October|November|December|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\b\"  # Match dates like 15th October\n","    }\n","\n","    # Placeholders for substitution\n","    placeholders = {\n","        \"url\": \"<URL>\",\n","        \"email\": \"<EMAIL>\",\n","        \"hashtag\": \"<HASHTAG>\",\n","        \"mention\": \"<MENTION>\",\n","        \"percentage\": \"<PERCENTAGE>\",\n","        \"age\": \"<AGE>\",\n","        \"time\": \"<TIME>\",\n","        \"number\": \"<NUMBER>\",\n","        \"date\": \"<DATE>\"\n","    }\n","\n","    for key, pattern in patterns.items():\n","        text = re.sub(pattern, placeholders[key], text)\n","    \n","    \n","    # Split text into tokens, keeping punctuation separate\n","    tokens = re.findall(r'\\b\\w+\\b|[.,!?;(){}\\[\\]\":\\'/-]', text)  # Match words and individual punctuation\n","    temp_placeholders=['URL', 'EMAIL', 'HASHTAG', 'MENTION','PERCENTAGE','AGE','TIME','NUMBER', 'DATE']\n","    \n","    # Clean up extra spaces around the tokens\n","    tokens = [token if token not in temp_placeholders else '<'+token+'>' for token in tokens]\n","    \n","    return tokens\n","\n","def tokenize_text(corpus):\n","        # Remove quotation marks before splitting into sentences\n","        corpus = re.sub(r'[\\'\"]', '', corpus)\n","        corpus = re.sub(r'[\\'_]', '', corpus)\n","        corpus = re.sub(r'[\\'-]', '', corpus)\n","        \n","\n","        # Split into sentences considering punctuation marks and the case of the next letter\n","        sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', corpus.strip())\n","        \n","        tokenized_sentences = [tokenize_sentences(sentence) for sentence in sentences]\n","        return tokenized_sentences\n","    \n","    \n","def read_corpus(file_path: str):\n","    if not os.path.exists(file_path):\n","        raise FileNotFoundError(f\"File not found: {file_path}\")\n","\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        text = file.read()\n","\n","    with open(\"output.txt\", 'w', encoding='utf-8') as file:\n","        file.write(text)\n","\n","    return tokenize_text(text)\n","\n","\n","def main():\n","    # sentences = read_corpus(\"./pride_and_prejudice.txt\")\n","    # for i in range(30):\n","    #     print (sentences[i])\n","    # print()\n","    print(\"Welcome to the Toknizer:\")\n","    while True:\n","        text = input(\"\\nEnter text to tokenize (or 'quit' to exit): \")\n","        if text.lower() == 'quit':\n","            print(\"Thank you for using tokenizer bye!\")\n","            break\n","\n","        tokens = tokenize_text(text)\n","        print(\"\\nTokenized text:\")\n","        print(tokens)\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2025-02-17T06:16:34.870111Z","iopub.status.busy":"2025-02-17T06:16:34.869753Z","iopub.status.idle":"2025-02-17T06:24:28.893487Z","shell.execute_reply":"2025-02-17T06:24:28.892657Z","shell.execute_reply.started":"2025-02-17T06:16:34.870084Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Training FFNN Model for 3-grams on iNLP/Pride_and_Prejudice.txt...\n","\n","Epoch 0 Loss:  0.1907750821006137\n","Epoch 1 Loss:  0.17088116370503403\n","Epoch 2 Loss:  0.16388738821552107\n","Epoch 3 Loss:  0.15874147350930187\n","Epoch 4 Loss:  0.1545520540613739\n","Model saved to trained_model_3_Pride_and_Prejudice.txt.pt\n","\n","Calculating and saving Train Perplexity...\n","Average perplexity of 3_Pride_and_Prejudice.txt_train_perplexity.txt:  80.16537281333902\n","\n","Calculating and saving Validation Perplexity...\n","Average perplexity of 3_Pride_and_Prejudice.txt_val_perplexity.txt:  207.3287804036289\n","\n","Calculating and saving Test Perplexity...\n","Average perplexity of 3_Pride_and_Prejudice.txt_test_perplexity.txt:  219.7100929023118\n","Perplexity scores saved in: 3_Pride_and_Prejudice.txt_train_perplexity.txt, 3_Pride_and_Prejudice.txt_val_perplexity.txt, 3_Pride_and_Prejudice.txt_test_perplexity.txt\n","\n","Training FFNN Model for 3-grams on iNLP/Ulysses.txt...\n","\n","Epoch 0 Loss:  0.23269224667824212\n","Epoch 1 Loss:  0.21672130941680667\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 292\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m5\u001b[39m]:\n\u001b[1;32m    291\u001b[0m     saved_models\u001b[38;5;241m.\u001b[39mappend(run_experiment(pride_corpus, n))\n\u001b[0;32m--> 292\u001b[0m     saved_models\u001b[38;5;241m.\u001b[39mappend(\u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mulysses_corpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSaved models:\u001b[39m\u001b[38;5;124m\"\u001b[39m, saved_models)\n","Cell \u001b[0;32mIn[4], line 243\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(corpus_path, n, model_type)\u001b[0m\n\u001b[1;32m    240\u001b[0m epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# if(n==5):\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m#     epoch=4\u001b[39;00m\n\u001b[0;32m--> 243\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[1;32m    246\u001b[0m model_save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrained_model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(corpus_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","Cell \u001b[0;32mIn[4], line 203\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, criterion, device, epoch)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 203\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Loss: \u001b[39m\u001b[38;5;124m\"\u001b[39m,total_loss \u001b[38;5;241m/\u001b[39m num_items)\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    214\u001b[0m         group,\n\u001b[1;32m    215\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m         state_steps,\n\u001b[1;32m    221\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:378\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    375\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[1;32m    377\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import random\n","from collections import defaultdict\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset, DataLoader\n","import os\n","# from tokenizer import Tokenizer\n","\n","def tokenize_corpus(corpus_path, n):\n","    with open(corpus_path, \"r\", encoding=\"utf-8\") as file:\n","        corpus = file.read()\n","\n","    tokenizer = Tokenizer(corpus)\n","    tokenized_sentences = tokenizer.tokenize()\n","\n","    # Add <s> and </s> tokens\n","    final_tokenized = []\n","    for sen in tokenized_sentences:\n","        final_tokenized.append([\"<s>\"] * (n-1) + sen + [\"</s>\"])\n","\n","    # Split dataset into train (80%), val (10%), test (10%)\n","    train_sentences, temp_sentences = train_test_split(final_tokenized, test_size=0.2, random_state=42)\n","    val_sentences, test_sentences = train_test_split(temp_sentences, test_size=0.5, random_state=42)\n","\n","    # Build Vocabulary (Add an <UNK> token for unseen words)\n","    vocab = {word: idx for idx, word in enumerate(set(word for sentence in train_sentences for word in sentence))}\n","    vocab[\"<UNK>\"] = len(vocab)  # Add <UNK> token\n","\n","    return train_sentences, val_sentences, test_sentences, vocab\n","\n","\n","\n","\n","class NGramDataset(Dataset):\n","    \"\"\" Dataset for N-Gram Language Model \"\"\"\n","    def __init__(self, tokenized_corpus, n, vocab):\n","        self.n = n\n","        self.vocab = vocab\n","        self.data = []\n","\n","        for sentence in tokenized_corpus:\n","            for i in range(len(sentence) - n):\n","                context = sentence[i:i + n - 1]\n","                target = sentence[i + n - 1]\n","                self.data.append((context, target, sentence))  # Store sentence here!\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        context, target, sentence = self.data[idx]\n","        context_tensor = torch.tensor([self.vocab.get(word, self.vocab[\"<UNK>\"]) for word in context], dtype=torch.long)\n","        target_tensor = torch.tensor(self.vocab.get(target, self.vocab[\"<UNK>\"]), dtype=torch.long)\n","        return context_tensor, target_tensor, \" \".join(sentence)  # Return full sentence as string\n","\n","\n","\n","class FFNNLanguageModel(nn.Module):\n","    \"\"\" Feed Forward Neural Network Language Model \"\"\"\n","    def __init__(self, vocab_size, embed_size, hidden_size, n,dropout_rate):\n","        super(FFNNLanguageModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        self.fc1 = nn.Linear((n-1) * embed_size, hidden_size)\n","        self.relu = nn.GELU()# TRY Gelu\n","        self.layer_norm = nn.LayerNorm(hidden_size)\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.fc2 = nn.Linear(hidden_size, vocab_size)\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        x = x.view(x.size(0), -1)  # Flatten\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","\n","        x = self.layer_norm(x)\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","        return self.softmax(x)\n","\n","\n","def calculate_nll(\n","    model: torch.nn.Module,\n","    loader: torch.utils.data.DataLoader,\n","    device: torch.device,\n",") -> tuple[list[float], list[str]]:\n","    model.eval()\n","\n","    sentence_perplexities, sentences = [], []\n","    with torch.no_grad():\n","        for context, target, batch_sentences in loader:\n","            context, target = context.to(device), target.to(device)\n","            # each of these are (batch_size, item)\n","            output = model(context)\n","            if output.size(0) != target.size(0):\n","                # Adjust output to match target size\n","                if output.size(0) < target.size(0):\n","                    target = target[: output.size(0)]\n","                else:\n","                    output = output[: target.size(0)]\n","\n","            loss = torch.nn.NLLLoss(reduction=\"none\")(output, target)\n","            num_sentences = len(loss)\n","            # append the perplexities one by one\n","            sentence_perplexities.extend(loss.cpu().numpy().tolist())\n","            sentences.extend(batch_sentences[:num_sentences])\n","    return sentence_perplexities, sentences\n","\n","import numpy as np\n","import torch\n","from torch.utils.data import DataLoader\n","from typing import List, Tuple\n","\n","def set_perplexity(\n","    model: torch.nn.Module,\n","    loader: DataLoader,\n","    device: torch.device,\n","    file_name: str,\n",") -> None:\n","    \"\"\"\n","    Computes and saves the perplexity of sentences in the given DataLoader.\n","\n","    Args:\n","    - model (torch.nn.Module): Trained model.\n","    - loader (DataLoader): DataLoader containing the dataset.\n","    - device (torch.device): CPU/GPU device.\n","    - file_name (str): Output file name to save perplexity scores.\n","    \"\"\"\n","    nll_losses, sentences = calculate_nll(model, loader, device)\n","    assert len(nll_losses) == len(\n","        sentences\n","    ), \"[set_perplexity] nll losses should be same length as sentences\"\n","\n","    sentence_perplexity = defaultdict(list)  \n","    \n","    for sentence, nll_loss in zip(sentences, nll_losses):\n","        sentence_perplexity[sentence].append(nll_loss)\n","        \n","    with open(file_name, \"w\") as f:\n","        for sentence in sentence_perplexity:\n","            perplexity = np.mean([np.exp(s) for s in sentence_perplexity[sentence]])\n","            f.write(f\"{sentence}\\t\\t\\t\\t{(perplexity)}\\n\")\n","\n","        average_perplexity = np.exp(sum(nll_losses) / len(nll_losses))\n","        f.write(f\"\\naverage perplexity: {average_perplexity}\\n\")\n","        print(f\"Average perplexity of {file_name}: \", average_perplexity)\n","\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","def train(\n","    model: torch.nn.Module,\n","    train_loader: torch.utils.data.DataLoader,\n","    optimizer: torch.optim.Optimizer,\n","    criterion: nn.Module,\n","    device: torch.device,\n","    epoch: int,\n",") -> float:\n","    \"\"\"\n","    Train the model for one epoch.\n","\n","    Args:\n","    - model (torch.nn.Module): The FFNN language model.\n","    - train_loader (DataLoader): DataLoader for training dataset.\n","    - optimizer (torch.optim.Optimizer): Optimizer for training.\n","    - criterion (nn.Module): Loss function (nn.NLLLoss).\n","    - device (torch.device): Device to train on (CPU/GPU).\n","\n","    Returns:\n","    - float: Average training loss per sample.\n","    \"\"\"\n","    num_items = len(train_loader.dataset)\n","    assert num_items > 0, \"[train] Training data must be present\"\n","    \n","    model.train()\n","    total_loss = 0\n","\n","    for e in range(epoch):\n","        total_loss=0 \n","        for context, target,_ in train_loader:  # Expecting dataset to return (context, target, sentence)\n","            context, target =/kaggle/input/corpus context.to(device), target.to(device)\n","            optimizer.zero_grad()\n","\n","            # Forward pass\n","            output = model(context)\n","\n","            # Ensure output and target sizes match\n","            if output.size(0) != target.size(0):\n","                if output.size(0) < target.size(0):\n","                    target = target[: output.size(0)]\n","                else:\n","                    output = output[: target.size(0)]\n","\n","            # Compute loss\n","            loss = criterion(output, target)\n","            # Backpropagation\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","        print(f\"Epoch {e} Loss: \",total_loss / num_items)\n","\n","    return model\n","\n","def run_experiment(corpus_path, n, model_type=\"FFNN\"):\n","    # Tokenize, build vocab, etc.\n","    train_sentences, val_sentences, test_sentences, vocab = tokenize_corpus(corpus_path, n)\n","\n","    # Create Datasets\n","    train_dataset = NGramDataset(train_sentences, n, vocab)\n","    val_dataset = NGramDataset(val_sentences, n, vocab)\n","    test_dataset = NGramDataset(test_sentences, n, vocab)\n","\n","    # Create DataLoaders\n","    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False, drop_last=False)\n","    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, drop_last=False)\n","    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, drop_last=False)\n","    \n","    # Initialize model\n","    vocab_size = len(vocab)\n","    embed_size = (300)\n","    hidden_size = (n*80)\n","\n","    model = FFNNLanguageModel(vocab_size, embed_size, hidden_size, n,0.6)\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","\n","     # Set up optimizer and loss function\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","    criterion = nn.NLLLoss()\n","\n","    # Train the model\n","    print(f\"\\nTraining FFNN Model for {n}-grams on {corpus_path}...\\n\")\n","    epoch=5\n","    # if(n==5):\n","    #     epoch=4\n","    model = train(model, train_loader, optimizer, criterion, device,epoch)\n","\n","    # Save the trained model\n","    model_save_path = f\"trained_model_{n}_{os.path.basename(corpus_path)}.pt\"\n","    torch.save({\n","        'model_state_dict': model.state_dict(),\n","        'vocab': vocab,  # Save the vocab so we can reuse it later\n","        'n': n\n","    }, model_save_path)\n","    print(f\"Model saved to {model_save_path}\")\n","    \n","    # Compute Sentence-Level Perplexity\n","    criterion = nn.CrossEntropyLoss(reduction='none')\n","    # Compute Perplexity for Train, Validation, and Test sets & Save to files\n","    train_file = f\"{n}_{corpus_path.split('/')[-1]}_train_perplexity.txt\"\n","    val_file = f\"{n}_{corpus_path.split('/')[-1]}_val_perplexity.txt\"\n","    test_file = f\"{n}_{corpus_path.split('/')[-1]}_test_perplexity.txt\"\n","\n","    print(\"\\nCalculating and saving Train Perplexity...\")\n","    set_perplexity(model, train_loader, device, train_file)\n","\n","    print(\"\\nCalculating and saving Validation Perplexity...\")\n","    set_perplexity(model, val_loader, device, val_file)\n","\n","    print(\"\\nCalculating and saving Test Perplexity...\")\n","    set_perplexity(model, test_loader, device, test_file)\n","\n","    print(f\"Perplexity scores saved in: {train_file}, {val_file}, {test_file}\")\n","\n","\n","    # If you also want to see the old approach (token-level) by summing everything:\n","    # train_ppl = compute_perplexity(model, train_loader, criterion, vocab, \"train_output.txt\")\n","    # test_ppl = compute_perplexity(model, test_loader, criterion, vocab, \"test_output.txt\")\n","    # print(f\"Token-Level Train PPL: {train_ppl:.4f}, Token-Level Test PPL: {test_ppl:.4f}\")\n","\n","    return model_save_path\n","\n","\n","\n","if __name__ == \"__main__\":\n","    # Corpus Paths\n","    pride_corpus = \"iNLP/Pride_and_Prejudice.txt\"\n","    ulysses_corpus = \"iNLP/Ulysses.txt\"\n","\n","    saved_models = []  # Store the saved model paths\n","\n","    # Run for n=3 and n=5\n","    for n in [3,5]:\n","        saved_models.append(run_experiment(pride_corpus, n))\n","        saved_models.append(run_experiment(ulysses_corpus, n))\n","\n","    print(\"\\nSaved models:\", saved_models)"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2025-02-16T14:33:22.489642Z","iopub.status.busy":"2025-02-16T14:33:22.489292Z","iopub.status.idle":"2025-02-16T14:34:17.269972Z","shell.execute_reply":"2025-02-16T14:34:17.269268Z","shell.execute_reply.started":"2025-02-16T14:33:22.489616Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-25-353fca10a9e1>:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(model_path, map_location=device)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  He is a very good \n"]},{"name":"stdout","output_type":"stream","text":["\n","Top predictions:\n","  job: 0.0431\n","  old: 0.0388\n","  day: 0.0307\n","  idea: 0.0277\n","  for: 0.0270\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  I am going to\n"]},{"name":"stdout","output_type":"stream","text":["\n","Top predictions:\n","  be: 0.0376\n","  throw: 0.0126\n","  see: 0.0084\n","  make: 0.0079\n","  catch: 0.0064\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  he is a nice\n"]},{"name":"stdout","output_type":"stream","text":["\n","Top predictions:\n","  man: 0.0371\n","  fellow: 0.0202\n","  young: 0.0139\n","  old: 0.0117\n","  little: 0.0085\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  She is a nice\n"]},{"name":"stdout","output_type":"stream","text":["\n","Top predictions:\n","  man: 0.0529\n","  fellow: 0.0200\n","  song: 0.0150\n","  young: 0.0131\n","  day: 0.0093\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  It is time to\n"]},{"name":"stdout","output_type":"stream","text":["\n","Top predictions:\n","  be: 0.0502\n","  the: 0.0334\n","  hell: 0.0244\n","  get: 0.0243\n","  see: 0.0111\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  I am going to see\n"]},{"name":"stdout","output_type":"stream","text":["\n","Top predictions:\n","  the: 0.2257\n","  them: 0.0555\n","  you: 0.0410\n","  her: 0.0355\n","  me: 0.0343\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  exit\n"]}],"source":["import torch\n","import torch.nn as nn\n","import sys\n","import os\n","import numpy as np\n","#from FFNN_language_model import FFNNLanguageModel  # Import FFNN model class\n","\n","def load_model(model_path, device):\n","    \"\"\"\n","    Loads a pre-trained FFNN language model from the given path.\n","\n","    Args:\n","    - model_path (str): Path to the trained model file.\n","    - device (torch.device): Device to load the model on (CPU/GPU).\n","\n","    Returns:\n","    - model (torch.nn.Module): Loaded FFNN model.\n","    - vocab (dict): Vocabulary used in training.\n","    - n (int): N-gram size used in training.\n","    \"\"\"\n","    checkpoint = torch.load(model_path, map_location=device)\n","\n","    vocab = checkpoint['vocab']\n","    n = checkpoint['n']\n","    vocab_size = len(vocab)\n","    embed_size = 300\n","    hidden_size = 256\n","\n","    # Initialize the model\n","    model = FFNNLanguageModel(vocab_size, embed_size, hidden_size, n, dropout_rate=0.5)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    model.to(device)\n","    model.eval()\n","\n","    return model, vocab, n\n","\n","def predict_next_words(model, vocab, n, input_text, k, device):\n","    \"\"\"\n","    Predicts the top K most probable next words given an input text.\n","\n","    Args:\n","    - model (torch.nn.Module): Loaded FFNN model.\n","    - vocab (dict): Vocabulary mapping words to indices.\n","    - n (int): N-gram size.\n","    - input_text (str): Input context for word prediction.\n","    - k (int): Number of top probable words to return.\n","    - device (torch.device): Device for model inference.\n","\n","    Returns:\n","    - List of tuples (word, probability) representing top-k predictions.\n","    \"\"\"\n","    # Tokenize input text\n","    words = input_text.strip().split()\n","\n","    if len(words) < n - 1:\n","        print(f\"[ERROR] Input must have at least {n-1} words.\")\n","        return []\n","\n","    # Extract last (n-1) words for context\n","    context_words = words[-(n-1):]\n","\n","    # Convert words to indices\n","    context_indices = [vocab.get(word, vocab[\"<UNK>\"]) for word in context_words]\n","    context_tensor = torch.tensor(context_indices, dtype=torch.long, device=device).unsqueeze(0)\n","\n","    # Get model predictions\n","    with torch.no_grad():\n","        output = model(context_tensor)  # Shape: (1, vocab_size)\n","        probabilities = torch.exp(output).squeeze(0).cpu().numpy()  # Convert log-probs to normal probs\n","\n","    # Get the top K words\n","    top_k_indices = np.argsort(probabilities)[-k:][::-1]  # Sort in descending order\n","    index_to_word = {idx: word for word, idx in vocab.items()}\n","    top_k_words = [(index_to_word[idx], probabilities[idx]) for idx in top_k_indices]\n","\n","    return top_k_words\n","\n","if __name__ == \"__main__\":\n","   \n","\n","    lm_type = \"-f\"  # FFNN model\n","    corpus_path = \"/kaggle/input/corpus/Ulysses.txt\"  # Path to corpus\n","    k = 5  # Number of top predicted words\n","\n","\n","    if lm_type != \"-f\":\n","        print(\"[ERROR] Currently, only FFNN models are supported.\")\n","        sys.exit(1)\n","\n","    # Find model file corresponding to the corpus\n","    model_path = f\"trained_model_5_{os.path.basename(corpus_path)}.pt\"\n","    if not os.path.exists(model_path):\n","        print(f\"[ERROR] Model file {model_path} not found.\")\n","        sys.exit(1)\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    \n","    # Load the trained model\n","    model, vocab, n = load_model(model_path, device)\n","\n","    # User input loop\n","    while True:\n","        input_text = input(\"\\nEnter input sentence (or type 'exit' to quit): \").strip()\n","        if input_text.lower() == \"exit\":\n","            break\n","\n","        top_k_words = predict_next_words(model, vocab, n, input_text, k, device)\n","        \n","        if top_k_words:\n","            print(\"\\nTop predictions:\")\n","            for word, prob in top_k_words:\n","                print(f\"  {word}: {prob:.4f}\")\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2025-02-13T12:39:25.697023Z","iopub.status.busy":"2025-02-13T12:39:25.696722Z","iopub.status.idle":"2025-02-13T12:42:40.471701Z","shell.execute_reply":"2025-02-13T12:42:40.470718Z","shell.execute_reply.started":"2025-02-13T12:39:25.697000Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Training RNN Model on /kaggle/input/corpus/Pride_and_Prejudice.txt...\n","\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-8-462487127e1f>:135: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n","<ipython-input-8-462487127e1f>:144: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1 - Average Loss: 5.8174\n","Epoch 2 - Average Loss: 5.0246\n","Epoch 3 - Average Loss: 4.5974\n","Epoch 4 - Average Loss: 4.2508\n","Epoch 5 - Average Loss: 3.9306\n","Model saved to trained_model_Pride_and_Prejudice.txt_RNN.pt\n","\n","Calculating and saving Train Perplexity...\n","Average perplexity of Pride_and_Prejudice.txt_train_perplexity_RNN.txt: 23.73036\n","\n","Calculating and saving Validation Perplexity...\n","Average perplexity of Pride_and_Prejudice.txt_val_perplexity_RNN.txt: 183.07515\n","\n","Calculating and saving Test Perplexity...\n","Average perplexity of Pride_and_Prejudice.txt_test_perplexity_RNN.txt: 188.38155\n","Perplexity scores saved in: Pride_and_Prejudice.txt_train_perplexity_RNN.txt, Pride_and_Prejudice.txt_val_perplexity_RNN.txt, Pride_and_Prejudice.txt_test_perplexity_RNN.txt\n","\n","Training RNN Model on /kaggle/input/corpus/Ulysses.txt...\n","\n","Epoch 1 - Average Loss: 7.0944\n","Epoch 2 - Average Loss: 6.4351\n","Epoch 3 - Average Loss: 6.0795\n","Epoch 4 - Average Loss: 5.7297\n","Epoch 5 - Average Loss: 5.3746\n","Model saved to trained_model_Ulysses.txt_RNN.pt\n","\n","Calculating and saving Train Perplexity...\n","Average perplexity of Ulysses.txt_train_perplexity_RNN.txt: 93.01260\n","\n","Calculating and saving Validation Perplexity...\n","Average perplexity of Ulysses.txt_val_perplexity_RNN.txt: 640.76157\n","\n","Calculating and saving Test Perplexity...\n","Average perplexity of Ulysses.txt_test_perplexity_RNN.txt: 639.54692\n","Perplexity scores saved in: Ulysses.txt_train_perplexity_RNN.txt, Ulysses.txt_val_perplexity_RNN.txt, Ulysses.txt_test_perplexity_RNN.txt\n","\n","Saved models: ['trained_model_Pride_and_Prejudice.txt_RNN.pt', 'trained_model_Ulysses.txt_RNN.pt']\n"]}],"source":["import os\n","import gc\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","from collections import defaultdict\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.cuda.amp import autocast, GradScaler\n","\n","# Optimize GPU usage and reduce fragmentation\n","torch.backends.cudnn.benchmark = True  \n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n","\n","# ---------------------------\n","# Data Preparation\n","# ---------------------------\n","MAX_SEQ_LENGTH = 128  # Maximum tokens per sentence\n","\n","def tokenize_corpus(corpus_path):\n","    \"\"\"\n","    Reads the corpus, tokenizes it, and splits long sentences.\n","    If a sentence is longer than MAX_SEQ_LENGTH, it is split into multiple segments.\n","    Each segment gets a start token \"<s>\" and an end token \"</s>\".\n","    \"\"\"\n","    with open(corpus_path, \"r\", encoding=\"utf-8\") as file:\n","        corpus = file.read()\n","    \n","    # Use your Tokenizer here; if not available, you can use a simple split.\n","    try:\n","        tokenizer = Tokenizer(corpus)  # Assumed to be defined elsewhere\n","        tokenized_sentences = tokenizer.tokenize()\n","    except Exception:\n","        tokenized_sentences = [line.split() for line in corpus.splitlines() if line.strip()]\n","    \n","    final_tokenized = []\n","    for sentence in tokenized_sentences:\n","        if len(sentence) > MAX_SEQ_LENGTH:\n","            for i in range(0, len(sentence), MAX_SEQ_LENGTH):\n","                sub_sentence = sentence[i:i + MAX_SEQ_LENGTH]\n","                final_tokenized.append([\"<s>\"] + sub_sentence + [\"</s>\"])\n","        else:\n","            final_tokenized.append([\"<s>\"] + sentence + [\"</s>\"])\n","    \n","    # Split into train (80%), validation (10%), test (10%)\n","    train_sentences, temp_sentences = train_test_split(final_tokenized, test_size=0.2, random_state=42)\n","    val_sentences, test_sentences = train_test_split(temp_sentences, test_size=0.5, random_state=42)\n","    \n","    # ---------------------------\n","    # Build Vocabulary with explicit <PAD> token at index 0.\n","    # ---------------------------\n","    vocab_set = set(word for sentence in train_sentences for word in sentence)\n","    vocab = {\"<PAD>\": 0}  # Reserve index 0 for padding.\n","    for word in vocab_set:\n","        if word != \"<PAD>\":\n","            vocab[word] = len(vocab)\n","    # Add <UNK> token if not already in vocab\n","    if \"<UNK>\" not in vocab:\n","        vocab[\"<UNK>\"] = len(vocab)\n","    \n","    return train_sentences, val_sentences, test_sentences, vocab\n","\n","# ---------------------------\n","# Dataset Definition\n","# ---------------------------\n","class SentenceDataset(Dataset):\n","    \"\"\"\n","    Dataset that returns entire sentences as sequences of token IDs.\n","    For each sentence, input is sentence[:-1] and target is sentence[1:].\n","    \"\"\"\n","    def __init__(self, tokenized_corpus, vocab):\n","        self.vocab = vocab\n","        self.sentences = tokenized_corpus  # Each sentence is a list of tokens\n","        \n","    def __len__(self):\n","        return len(self.sentences)\n","    \n","    def __getitem__(self, idx):\n","        sentence = self.sentences[idx]\n","        input_tokens = [self.vocab.get(word, self.vocab[\"<UNK>\"]) for word in sentence[:-1]]\n","        target_tokens = [self.vocab.get(word, self.vocab[\"<UNK>\"]) for word in sentence[1:]]\n","        return (torch.tensor(input_tokens, dtype=torch.long),\n","                torch.tensor(target_tokens, dtype=torch.long),\n","                \" \".join(sentence))\n","\n","# ---------------------------\n","# Collate Function for Padding\n","# ---------------------------\n","def collate_fn(batch):\n","    \"\"\"\n","    Pads sequences in a batch to the same length.\n","    Each item in the batch is (input_tensor, target_tensor, sentence_string).\n","    \"\"\"\n","    inputs = [item[0] for item in batch]\n","    targets = [item[1] for item in batch]\n","    sentences = [item[2] for item in batch]\n","    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=0)\n","    targets_padded = pad_sequence(targets, batch_first=True, padding_value=0)\n","    return inputs_padded, targets_padded, sentences\n","\n","# ---------------------------\n","# Vanilla RNN Language Model Definition\n","# ---------------------------\n","class RNNLanguageModel(nn.Module):\n","    \"\"\" RNN-based language model for full-sentence prediction. \"\"\"\n","    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1, dropout_rate=0.5):\n","        super(RNNLanguageModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        self.rnn = nn.RNN(embed_size, hidden_size, num_layers=num_layers, batch_first=True)\n","        self.relu = nn.Tanh()  # Using GELU; you can experiment with other activations.\n","        self.layer_norm = nn.LayerNorm(hidden_size)\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.fc = nn.Linear(hidden_size, vocab_size)\n","        self.log_softmax = nn.LogSoftmax(dim=1)\n","    \n","    def forward(self, x, hidden=None):\n","        embedded = self.embedding(x)  # (batch_size, seq_length, embed_size)\n","        output, hidden = self.rnn(embedded, hidden)  # (batch_size, seq_length, hidden_size)\n","        output = self.layer_norm(output)\n","        output = self.dropout(output)\n","        logits = self.fc(output)  # (batch_size, seq_length, vocab_size)\n","        batch_size, seq_len, _ = logits.size()\n","        logits = logits.contiguous().view(batch_size * seq_len, -1)\n","        log_probs = self.log_softmax(logits)\n","        return log_probs, hidden\n","\n","# ---------------------------\n","# Training Function (Using AMP)\n","# ---------------------------\n","def train(model, train_loader, optimizer, criterion, device, epochs):\n","    \"\"\" Trains the RNN/LSTM model using mixed precision (AMP) and ignores padding tokens. \"\"\"\n","    model.train()\n","    scaler = GradScaler()\n","\n","    for epoch in range(epochs):\n","        epoch_loss, total_samples = 0, 0\n","    \n","        for inputs, targets, _ in train_loader:\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            optimizer.zero_grad()\n","    \n","            with autocast():\n","                log_probs, _ = model(inputs)\n","                flat_targets = targets.contiguous().view(-1)\n","                # With reduction=\"none\", loss is a vector of losses per token\n","                loss = criterion(log_probs, flat_targets)\n","                mask = flat_targets != 0  # Assuming 0 is the <PAD> index\n","                masked_loss = loss[mask]   # Now we can index since loss is not a scalar\n","                if masked_loss.numel() > 0:\n","                    loss = masked_loss.mean()\n","                else:\n","                    loss = torch.tensor(0.0, device=device)\n","    \n","            scaler.scale(loss).backward()\n","            scaler.step(optimizer)\n","            scaler.update()\n","    \n","            epoch_loss += loss.item() * mask.sum().item()\n","            total_samples += mask.sum().item()\n","    \n","        avg_loss = epoch_loss / total_samples if total_samples > 0 else float('inf')\n","        print(f\"Epoch {epoch+1} - Average Loss: {avg_loss:.4f}\")\n","    \n","    return model\n","\n","\n","\n","# ---------------------------\n","# Perplexity Calculation Functions\n","# ---------------------------\n","def calculate_nll(model, loader, device) -> tuple[list[float], list[str]]:\n","    \"\"\"\n","    Compute the negative log-likelihood (NLL) loss for each sentence.\n","    For each sentence, flatten predictions and targets and compute mean loss.\n","    \"\"\"\n","    model.eval()\n","    sentence_losses = []\n","    sentences_out = []\n","    \n","    with torch.no_grad():\n","        for inputs, targets, sentences in loader:\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            log_probs, _ = model(inputs)  # (batch_size * seq_length, vocab_size)\n","            flat_targets = targets.contiguous().view(-1)\n","            loss = nn.NLLLoss(reduction=\"none\")(log_probs, flat_targets)\n","            batch_size, seq_len = inputs.size(0), inputs.size(1)\n","            loss = loss.view(batch_size, seq_len)\n","            for i in range(batch_size):\n","                # Create mask for non-padding tokens (padding is 0, reserved for <PAD>)\n","                mask = targets[i] != 0\n","                if mask.sum().item() > 0:\n","                    mean_loss = loss[i][mask].mean().item()\n","                else:\n","                    mean_loss = float('inf')\n","                sentence_losses.append(mean_loss)\n","            sentences_out.extend(sentences)\n","    return sentence_losses, sentences_out\n","\n","def set_perplexity(model, loader, device, file_name):\n","    \"\"\"\n","    Computes and saves the perplexity of sentences in the given DataLoader.\n","    Duplicate sentences are merged by averaging their losses.\n","    \"\"\"\n","    nll_losses, sentences = calculate_nll(model, loader, device)\n","    from collections import defaultdict\n","    sentence_loss_dict = defaultdict(list)\n","    for sentence, loss in zip(sentences, nll_losses):\n","        sentence_loss_dict[sentence].append(loss)\n","    with open(file_name, \"w\") as f:\n","        f.write(f\"{'Sentence':<80}{'Perplexity'}\\n\")\n","        f.write(\"=\" * 100 + \"\\n\")\n","        for sentence, losses in sentence_loss_dict.items():\n","            avg_loss = np.mean(losses)\n","            ppl = np.exp(avg_loss)\n","            f.write(f\"{sentence:<80}{ppl:.5f}\\n\")\n","        overall_ppl = np.exp(np.mean([np.mean(losses) for losses in sentence_loss_dict.values()]))\n","        f.write(\"=\" * 100 + \"\\n\")\n","        f.write(f\"{'Average Perplexity':<80}{overall_ppl:.5f}\\n\")\n","        print(f\"Average perplexity of {file_name}: {overall_ppl:.5f}\")\n","\n","# ---------------------------\n","# Run Experiment Function\n","# ---------------------------\n","def run_experiment(corpus_path, model_type=\"RNN\"):\n","    \"\"\"\n","    Runs the full training and evaluation pipeline.\n","    \"\"\"\n","    # Tokenize and prepare data\n","    train_sentences, val_sentences, test_sentences, vocab = tokenize_corpus(corpus_path)\n","    \n","    train_dataset = SentenceDataset(train_sentences, vocab)\n","    val_dataset = SentenceDataset(val_sentences, vocab)\n","    test_dataset = SentenceDataset(test_sentences, vocab)\n","    \n","    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, drop_last=False, collate_fn=collate_fn, pin_memory=True)\n","    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, drop_last=False, collate_fn=collate_fn, pin_memory=True)\n","    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, drop_last=False, collate_fn=collate_fn, pin_memory=True)\n","    \n","    vocab_size = len(vocab)\n","    embed_size = 300\n","    hidden_size = 512\n","    \n","    # Create RNN Model\n","    model = RNNLanguageModel(vocab_size, embed_size, hidden_size, num_layers=1, dropout_rate=0.3)\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","    \n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","    criterion = nn.NLLLoss(ignore_index=0,reduction=\"none\")  # 0 is the index of <PAD> token in the vocab\n","    \n","    print(f\"\\nTraining {model_type} Model on {corpus_path}...\\n\")\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    model = train(model, train_loader, optimizer, criterion, device, epochs=5)\n","    \n","    model_save_path = f\"trained_model_{os.path.basename(corpus_path)}_{model_type}.pt\"\n","    torch.save({'model_state_dict': model.state_dict(), 'vocab': vocab}, model_save_path)\n","    print(f\"Model saved to {model_save_path}\")\n","    \n","    train_file = f\"{os.path.basename(corpus_path)}_train_perplexity_{model_type}.txt\"\n","    val_file = f\"{os.path.basename(corpus_path)}_val_perplexity_{model_type}.txt\"\n","    test_file = f\"{os.path.basename(corpus_path)}_test_perplexity_{model_type}.txt\"\n","    \n","    print(\"\\nCalculating and saving Train Perplexity...\")\n","    set_perplexity(model, train_loader, device, train_file)\n","    print(\"\\nCalculating and saving Validation Perplexity...\")\n","    set_perplexity(model, val_loader, device, val_file)\n","    print(\"\\nCalculating and saving Test Perplexity...\")\n","    set_perplexity(model, test_loader, device, test_file)\n","    print(f\"Perplexity scores saved in: {train_file}, {val_file}, {test_file}\")\n","    \n","    return model_save_path\n","\n","# ---------------------------\n","# Main\n","# ---------------------------\n","if __name__ == \"__main__\":\n","    pride_corpus = \"/kaggle/input/corpus/Pride_and_Prejudice.txt\"\n","    ulysses_corpus = \"/kaggle/input/corpus/Ulysses.txt\"\n","    \n","    saved_models = []\n","    for corpus in [pride_corpus, ulysses_corpus]:\n","        saved_models.append(run_experiment(corpus, model_type=\"RNN\"))\n","    \n","    print(\"\\nSaved models:\", saved_models)\n"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2025-02-13T12:49:36.000414Z","iopub.status.busy":"2025-02-13T12:49:36.000071Z","iopub.status.idle":"2025-02-13T12:52:14.805902Z","shell.execute_reply":"2025-02-13T12:52:14.805213Z","shell.execute_reply.started":"2025-02-13T12:49:36.000389Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-15-4663e81545c7>:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(model_path, map_location=device)\n"]},{"name":"stdout","output_type":"stream","text":["Enter an input sentence:  He is a \n"]},{"name":"stdout","output_type":"stream","text":["RNN Next-Word Generator\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  I am\n"]},{"name":"stdout","output_type":"stream","text":["\n","Top predictions:\n","  entitled: 0.0356\n","  convinced: 0.0297\n","  determined: 0.0259\n","  glad: 0.0236\n","  gone: 0.0221\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  He is\n"]},{"name":"stdout","output_type":"stream","text":["\n","Top predictions:\n","  exquisite: 0.1613\n","  right: 0.0183\n","  blessed: 0.0144\n","  recovered: 0.0142\n","  mercenary: 0.0132\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  He is a\n"]},{"name":"stdout","output_type":"stream","text":["\n","Top predictions:\n","  joke: 0.1265\n","  son: 0.0870\n","  wide: 0.0624\n","  most: 0.0458\n","  stubbornness: 0.0275\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  He is the most\n"]},{"name":"stdout","output_type":"stream","text":["\n","Top predictions:\n","  amusing: 0.1953\n","  unforgiving: 0.1466\n","  flattering: 0.0367\n","  remarkable: 0.0210\n","  elevated: 0.0202\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  She is a very good\n"]},{"name":"stdout","output_type":"stream","text":["\n","Top predictions:\n","  fun: 0.1202\n","  girl: 0.1104\n","  creature: 0.0336\n","  distance: 0.0327\n","  sort: 0.0287\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  He is a very good\n"]},{"name":"stdout","output_type":"stream","text":["\n","Top predictions:\n","  fun: 0.1202\n","  girl: 0.1104\n","  creature: 0.0336\n","  distance: 0.0327\n","  sort: 0.0287\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  We are a nice\n"]},{"name":"stdout","output_type":"stream","text":["\n","Top predictions:\n","  long: 0.1220\n","  which: 0.0475\n","  aspect: 0.0365\n","  and: 0.0361\n","  of: 0.0334\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  I am going to\n"]},{"name":"stdout","output_type":"stream","text":["\n","Top predictions:\n","  be: 0.0867\n","  make: 0.0623\n","  introduce: 0.0601\n","  sit: 0.0434\n","  hope: 0.0309\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  Why are you\n"]},{"name":"stdout","output_type":"stream","text":["\n","Top predictions:\n","  must: 0.1006\n","  have: 0.0348\n","  pleased: 0.0314\n","  know: 0.0288\n","  are: 0.0266\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  He is the most\n"]},{"name":"stdout","output_type":"stream","text":["\n","Top predictions:\n","  amusing: 0.1953\n","  unforgiving: 0.1466\n","  flattering: 0.0367\n","  remarkable: 0.0210\n","  elevated: 0.0202\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  he is the\n"]},{"name":"stdout","output_type":"stream","text":["\n","Top predictions:\n","  most: 0.2013\n","  handsomest: 0.0938\n","  very: 0.0296\n","  same: 0.0267\n","  son: 0.0223\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  she is the\n"]},{"name":"stdout","output_type":"stream","text":["\n","Top predictions:\n","  most: 0.0434\n","  youngest: 0.0404\n","  greatest: 0.0248\n","  handsomest: 0.0239\n","  same: 0.0220\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  he is the handsomest young\n"]},{"name":"stdout","output_type":"stream","text":["\n","Top predictions:\n","  man: 0.6015\n","  woman: 0.1477\n","  ladies: 0.0795\n","  men: 0.0451\n","  lady: 0.0374\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  he is the handsomest young man that ever was\n"]},{"name":"stdout","output_type":"stream","text":["\n","Top predictions:\n","  not: 0.1076\n","  a: 0.0729\n","  the: 0.0557\n","  over: 0.0487\n","  so: 0.0383\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  exit\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","def load_model(model_path, device):\n","    \"\"\"\n","    Loads a pre-trained RNN language model from a checkpoint.\n","    \n","    Args:\n","      model_path (str): Path to the trained model file.\n","      device (torch.device): Device to load the model on (CPU/GPU).\n","      \n","    Returns:\n","      model (torch.nn.Module): Loaded RNN model.\n","      vocab (dict): Vocabulary used in training.\n","    \"\"\"\n","    checkpoint = torch.load(model_path, map_location=device)\n","    \n","    vocab = checkpoint['vocab']\n","    vocab_size = len(vocab)\n","    embed_size = 300    # Ensure this matches the training config\n","    hidden_size = 512   # Ensure this matches the training config\n","    num_layers = 1      # As used during training\n","    dropout_rate = 0.3  # As used during training\n","\n","    # Initialize the model\n","    model = RNNLanguageModel(vocab_size, embed_size, hidden_size, num_layers, dropout_rate)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    model.to(device)\n","    model.eval()  # Set to evaluation mode\n","\n","    return model, vocab\n","\n","\n","def predict_next_words(model, vocab, input_sentence, k, device):\n","    \"\"\"\n","    Given an input sentence, returns the top k predicted next words along with their probabilities.\n","    \n","    Args:\n","      model (torch.nn.Module): The trained RNN language model.\n","      vocab (dict): Vocabulary mapping words to indices.\n","      input_sentence (str): The input sentence (context) as a string.\n","      k (int): Number of top predicted words to return.\n","      device (torch.device): The device on which to run the inference.\n","      \n","    Returns:\n","      List[Tuple[str, float]]: A list of tuples where each tuple is (word, probability).\n","    \"\"\"\n","    # Tokenize the input sentence (using simple whitespace splitting)\n","    words = input_sentence.strip().split()\n","    if not words:\n","        print(\"[ERROR] Input sentence is empty.\")\n","        return []\n","    \n","    # Convert words to indices (using <UNK> for out-of-vocabulary words)\n","    indices = [vocab.get(word, vocab[\"<UNK>\"]) for word in words]\n","    \n","    # Create input tensor with shape (1, seq_length)\n","    input_tensor = torch.tensor(indices, dtype=torch.long, device=device).unsqueeze(0)\n","    \n","    with torch.no_grad():\n","        # Get model output; our model returns flattened log_probs, so we reshape it\n","        log_probs, _ = model(input_tensor)\n","    \n","    # Determine the sequence length from the input tensor\n","    seq_length = input_tensor.size(1)\n","    vocab_size = len(vocab)\n","    \n","    # Reshape log_probs from (batch_size*seq_length, vocab_size) to (batch_size, seq_length, vocab_size)\n","    log_probs = log_probs.view(1, seq_length, vocab_size)\n","    \n","    # We want to use the prediction for the last time step (i.e. next word prediction)\n","    last_log_probs = log_probs[:, -1, :]  # Shape: (1, vocab_size)\n","    probabilities = torch.exp(last_log_probs).squeeze(0)  # Shape: (vocab_size,)\n","    \n","    # Get the top k predictions, sorted in descending order\n","    topk = torch.topk(probabilities, k, largest=True, sorted=True)\n","    topk_indices = topk.indices.cpu().numpy()\n","    topk_probs = topk.values.cpu().numpy()\n","    \n","    # Create an inverse mapping from index to word\n","    index_to_word = {idx: word for word, idx in vocab.items()}\n","    \n","    predictions = [(index_to_word.get(int(idx), \"<UNK>\"), float(prob)) for idx, prob in zip(topk_indices, topk_probs)]\n","    \n","    return predictions\n","\n","# Example usage:\n","if __name__ == \"__main__\":\n","    # Assume model and vocab are already loaded, for example:\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model, vocab = load_model(\"trained_model_Pride_and_Prejudice.txt_RNN.pt\", device)  # Your loading function\n","    # For demonstration, we'll assume they are defined.\n","    \n","    input_sentence = input(\"Enter an input sentence: \").strip()\n","    k = 5\n","    print(\"RNN Next-Word Generator\")\n","    while True:\n","        input_text = input(\"\\nEnter input sentence (or type 'exit' to quit): \").strip()\n","        if input_text.lower() == \"exit\":\n","            break\n","        preds = predict_next_words(model, vocab, input_text, k, device)\n","        print(\"\\nTop predictions:\")\n","        for word, prob in preds:\n","            print(f\"  {word}: {prob:.4f}\")\n"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2025-02-13T12:52:19.909825Z","iopub.status.busy":"2025-02-13T12:52:19.909516Z","iopub.status.idle":"2025-02-13T12:54:56.862796Z","shell.execute_reply":"2025-02-13T12:54:56.861810Z","shell.execute_reply.started":"2025-02-13T12:52:19.909803Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Training LSTM Model on /kaggle/input/corpus/Pride_and_Prejudice.txt...\n","\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-16-5d5f6a8efc6f>:166: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n","<ipython-input-16-5d5f6a8efc6f>:175: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1 - Average Loss: 5.7155\n","Epoch 2 - Average Loss: 4.9206\n","Epoch 3 - Average Loss: 4.4996\n","Epoch 4 - Average Loss: 4.1361\n","Epoch 5 - Average Loss: 3.7850\n","Model saved to trained_model_Pride_and_Prejudice.txt_LSTM.pt\n","\n","Calculating and saving Train Perplexity...\n","Average perplexity of Pride_and_Prejudice.txt_train_perplexity_LSTM.txt: 19.25903\n","\n","Calculating and saving Validation Perplexity...\n","Average perplexity of Pride_and_Prejudice.txt_val_perplexity_LSTM.txt: 155.25994\n","\n","Calculating and saving Test Perplexity...\n","Average perplexity of Pride_and_Prejudice.txt_test_perplexity_LSTM.txt: 161.31459\n","Perplexity scores saved in: Pride_and_Prejudice.txt_train_perplexity_LSTM.txt, Pride_and_Prejudice.txt_val_perplexity_LSTM.txt, Pride_and_Prejudice.txt_test_perplexity_LSTM.txt\n","\n","Training LSTM Model on /kaggle/input/corpus/Ulysses.txt...\n","\n","Epoch 1 - Average Loss: 6.9025\n","Epoch 2 - Average Loss: 6.2322\n","Epoch 3 - Average Loss: 5.7683\n","Epoch 4 - Average Loss: 5.3127\n","Epoch 5 - Average Loss: 4.8855\n","Model saved to trained_model_Ulysses.txt_LSTM.pt\n","\n","Calculating and saving Train Perplexity...\n","Average perplexity of Ulysses.txt_train_perplexity_LSTM.txt: 47.68135\n","\n","Calculating and saving Validation Perplexity...\n","Average perplexity of Ulysses.txt_val_perplexity_LSTM.txt: 630.35080\n","\n","Calculating and saving Test Perplexity...\n","Average perplexity of Ulysses.txt_test_perplexity_LSTM.txt: 626.64241\n","Perplexity scores saved in: Ulysses.txt_train_perplexity_LSTM.txt, Ulysses.txt_val_perplexity_LSTM.txt, Ulysses.txt_test_perplexity_LSTM.txt\n","\n","Saved models: ['trained_model_Pride_and_Prejudice.txt_LSTM.pt', 'trained_model_Ulysses.txt_LSTM.pt']\n"]}],"source":["import os\n","import gc\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","from collections import defaultdict\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.cuda.amp import autocast, GradScaler\n","\n","# Uncomment or define your Tokenizer class as needed.\n","# from tokenizer import Tokenizer\n","\n","# For better GPU memory management\n","torch.backends.cudnn.benchmark = True\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n","\n","# ---------------------------\n","# Data Preparation\n","# ---------------------------\n","MAX_SEQ_LENGTH = 128  # Maximum tokens per sentence\n","\n","def tokenize_corpus(corpus_path):\n","    \"\"\"\n","    Reads the corpus, tokenizes it, and ensures that no sentence exceeds MAX_SEQ_LENGTH.\n","    If a sentence is longer, it is split into multiple sentences of length MAX_SEQ_LENGTH.\n","    Start (\"<s>\") and end (\"</s>\") tokens are added to each segment.\n","    \"\"\"\n","    with open(corpus_path, \"r\", encoding=\"utf-8\") as file:\n","        corpus = file.read()\n","    \n","    tokenizer = Tokenizer(corpus)  # Assuming Tokenizer is defined elsewhere\n","    tokenized_sentences = tokenizer.tokenize()\n","\n","    final_tokenized = []\n","    \n","    for sentence in tokenized_sentences:\n","        if len(sentence) > MAX_SEQ_LENGTH:\n","            # Split into multiple sentences of length MAX_SEQ_LENGTH\n","            for i in range(0, len(sentence), MAX_SEQ_LENGTH):\n","                sub_sentence = sentence[i:i + MAX_SEQ_LENGTH]  # Take 128 tokens at a time\n","                final_tokenized.append([\"<s>\"] + sub_sentence + [\"</s>\"])\n","        else:\n","            # Keep sentence as is if within length limit\n","            final_tokenized.append([\"<s>\"] + sentence + [\"</s>\"])\n","    \n","    # Split dataset: 80% Train, 10% Validation, 10% Test\n","    train_sentences, temp_sentences = train_test_split(final_tokenized, test_size=0.2, random_state=42)\n","    val_sentences, test_sentences = train_test_split(temp_sentences, test_size=0.5, random_state=42)\n","    \n","    # Build vocabulary from training sentences; add <UNK> token\n","    vocab = {word: idx for idx, word in enumerate(set(word for sentence in train_sentences for word in sentence))}\n","    vocab[\"<UNK>\"] = len(vocab)\n","    \n","    return train_sentences, val_sentences, test_sentences, vocab\n","\n","# ---------------------------\n","# Dataset Definition (Full Sentence)\n","# ---------------------------\n","class SentenceDataset(Dataset):\n","    \"\"\"\n","    Dataset that returns entire sentences as sequences of token IDs.\n","    For each sentence, the input is sentence[:-1] and the target is sentence[1:].\n","    \"\"\"\n","    def __init__(self, tokenized_corpus, vocab):\n","        self.vocab = vocab\n","        self.sentences = tokenized_corpus  # Each sentence is a list of tokens\n","        \n","    def __len__(self):\n","        return len(self.sentences)\n","    \n","    def __getitem__(self, idx):\n","        sentence = self.sentences[idx]\n","        input_tokens = [self.vocab.get(word, self.vocab[\"<UNK>\"]) for word in sentence[:-1]]\n","        target_tokens = [self.vocab.get(word, self.vocab[\"<UNK>\"]) for word in sentence[1:]]\n","        return (torch.tensor(input_tokens, dtype=torch.long),\n","                torch.tensor(target_tokens, dtype=torch.long),\n","                \" \".join(sentence))\n","\n","# ---------------------------\n","# Collate Function for Padding\n","# ---------------------------\n","def collate_fn(batch):\n","    \"\"\"\n","    Pads sequences in a batch.\n","    Each item in the batch is (input_tensor, target_tensor, sentence_string).\n","    \"\"\"\n","    inputs = [item[0] for item in batch]\n","    targets = [item[1] for item in batch]\n","    sentences = [item[2] for item in batch]\n","    \n","    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=0)\n","    targets_padded = pad_sequence(targets, batch_first=True, padding_value=0)\n","    return inputs_padded, targets_padded, sentences\n","\n","# ---------------------------\n","# Vanilla LSTM Language Model Definition\n","# ---------------------------\n","# ---------------------------\n","# Vanilla LSTM Language Model Definition\n","# ---------------------------\n","class LSTMLanguageModel(nn.Module):\n","    \"\"\"\n","    Vanilla LSTM Language Model.\n","    \n","    The model embeds input tokens, processes the sequence with an LSTM,\n","    and uses the final hidden state to predict the next word.\n","    \n","    Args:\n","      vocab_size (int): Size of the vocabulary.\n","      embed_size (int): Dimensionality of word embeddings.\n","      hidden_size (int): Size of the LSTM hidden state.\n","      num_layers (int): Number of LSTM layers.\n","      dropout_rate (float): Dropout rate.\n","    \"\"\"\n","    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1, dropout_rate=0.5):\n","        super(LSTMLanguageModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=num_layers, batch_first=True)\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.fc = nn.Linear(hidden_size, vocab_size)\n","        self.log_softmax = nn.LogSoftmax(dim=1)\n","        self.layer_norm = nn.LayerNorm(hidden_size)\n","        self.num_layers = num_layers\n","        self.hidden_size = hidden_size\n","\n","    def forward(self, x, hidden=None):\n","        \"\"\"\n","        Args:\n","          x (Tensor): Input tensor of shape (batch_size, seq_length)\n","          hidden (tuple): Optional initial (hidden, cell) state.\n","          \n","        Returns:\n","          log_probs (Tensor): Log-probabilities with shape (batch_size*seq_length, vocab_size)\n","          hidden (tuple): Final (hidden, cell) state.\n","        \"\"\"\n","        batch_size = x.size(0)  # Get batch size dynamically\n","\n","        # Initialize hidden and cell states if not provided\n","        if hidden is None:\n","            h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=x.device)\n","            c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=x.device)\n","            hidden = (h0, c0)\n","\n","        embedded = self.embedding(x)  # (batch_size, seq_length, embed_size)\n","        output, hidden = self.lstm(embedded, hidden)  # (batch_size, seq_length, hidden_size)\n","        output = self.layer_norm(output)\n","        output = self.dropout(output)\n","        logits = self.fc(output)  # (batch_size, seq_length, vocab_size)\n","        \n","        batch_size, seq_len, _ = logits.size()\n","        logits = logits.contiguous().view(batch_size * seq_len, -1)  # Flatten for loss computation\n","        log_probs = self.log_softmax(logits)\n","\n","        return log_probs, hidden\n","\n","\n","# ---------------------------\n","# Training Function (Using AMP)\n","# ---------------------------\n","def train(model, train_loader, optimizer, criterion, device, epochs):\n","    \"\"\" Trains the RNN/LSTM model using mixed precision (AMP) and ignores padding tokens. \"\"\"\n","    model.train()\n","    scaler = GradScaler()\n","\n","    for epoch in range(epochs):\n","        epoch_loss, total_samples = 0, 0\n","    \n","        for inputs, targets, _ in train_loader:\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            optimizer.zero_grad()\n","    \n","            with autocast():\n","                log_probs, _ = model(inputs)\n","                flat_targets = targets.contiguous().view(-1)\n","                # With reduction=\"none\", loss is a vector of losses per token\n","                loss = criterion(log_probs, flat_targets)\n","                mask = flat_targets != 0  # Assuming 0 is the <PAD> index\n","                masked_loss = loss[mask]   # Now we can index since loss is not a scalar\n","                if masked_loss.numel() > 0:\n","                    loss = masked_loss.mean()\n","                else:\n","                    loss = torch.tensor(0.0, device=device)\n","    \n","            scaler.scale(loss).backward()\n","            scaler.step(optimizer)\n","            scaler.update()\n","    \n","            epoch_loss += loss.item() * mask.sum().item()\n","            total_samples += mask.sum().item()\n","    \n","        avg_loss = epoch_loss / total_samples if total_samples > 0 else float('inf')\n","        print(f\"Epoch {epoch+1} - Average Loss: {avg_loss:.4f}\")\n","    \n","    return model\n","\n","# ---------------------------\n","# Perplexity Calculation Functions\n","# ---------------------------\n","def calculate_nll(model, loader, device):\n","    \"\"\"\n","    Compute the negative log-likelihood (NLL) loss for each sentence.\n","    For each sentence, flatten predictions and targets and compute mean loss.\n","    \"\"\"\n","    model.eval()\n","    sentence_losses = []\n","    sentences_out = []\n","    \n","    with torch.no_grad():\n","        for inputs, targets, sentences in loader:\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            log_probs, _ = model(inputs)  # (batch_size * seq_len, vocab_size)\n","            flat_targets = targets.contiguous().view(-1)\n","            loss = nn.NLLLoss(reduction=\"none\")(log_probs, flat_targets)\n","            # Reshape loss to (batch_size, seq_len)\n","            batch_size, seq_len = inputs.size(0), inputs.size(1)\n","            loss = loss.view(batch_size, seq_len)\n","            for i in range(batch_size):\n","                # Create mask for non-padding tokens (assuming padding value 0)\n","                mask = targets[i] != 0\n","                if mask.sum().item() > 0:\n","                    mean_loss = loss[i][mask].mean().item()\n","                else:\n","                    mean_loss = float('inf')\n","                sentence_losses.append(mean_loss)\n","            sentences_out.extend(sentences)\n","    return sentence_losses, sentences_out\n","\n","def set_perplexity(model, loader, device, file_name):\n","    \"\"\"\n","    Computes and saves sentence-level perplexity.\n","    Duplicate sentences are merged by averaging their losses.\n","    \"\"\"\n","    nll_losses, sentences = calculate_nll(model, loader, device)\n","    # Merge duplicate sentences\n","    loss_dict = defaultdict(list)\n","    for sent, loss in zip(sentences, nll_losses):\n","        loss_dict[sent].append(loss)\n","    \n","    with open(file_name, \"w\") as f:\n","        f.write(f\"{'Sentence':<80}{'Perplexity'}\\n\")\n","        f.write(\"=\" * 100 + \"\\n\")\n","        for sent, losses in loss_dict.items():\n","            avg_loss = np.mean(losses)\n","            ppl = np.exp(avg_loss)\n","            f.write(f\"{sent:<80}{ppl:.5f}\\n\")\n","        overall_ppl = np.exp(np.mean([np.mean(losses) for losses in loss_dict.values()]))\n","        f.write(\"=\" * 100 + \"\\n\")\n","        f.write(f\"{'Average Perplexity':<80}{overall_ppl:.5f}\\n\")\n","        print(f\"Average perplexity of {file_name}: {overall_ppl:.5f}\")\n","\n","# ---------------------------\n","# Run Experiment (Training and Evaluation)\n","# ---------------------------\n","def run_experiment(corpus_path, model_type=\"LSTM\"):\n","    # Tokenize and prepare data\n","    train_sentences, val_sentences, test_sentences, vocab = tokenize_corpus(corpus_path)\n","    train_dataset = SentenceDataset(train_sentences, vocab)\n","    val_dataset = SentenceDataset(val_sentences, vocab)\n","    test_dataset = SentenceDataset(test_sentences, vocab)\n","    \n","    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, drop_last=False, collate_fn=collate_fn, pin_memory=True)\n","    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, drop_last=False, collate_fn=collate_fn, pin_memory=True)\n","    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, drop_last=False, collate_fn=collate_fn, pin_memory=True)\n","    \n","    vocab_size = len(vocab)\n","    embed_size = 300\n","    hidden_size = 256\n","    \n","    # Create LSTM model instance\n","    model = LSTMLanguageModel(vocab_size, embed_size, hidden_size, num_layers=1, dropout_rate=0.3)\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","    \n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","    criterion = nn.NLLLoss(ignore_index=0,reduction=\"none\")  # 0 is the index of <PAD> token in the vocab\n","    \n","    print(f\"\\nTraining {model_type} Model on {corpus_path}...\\n\")\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    model = train(model, train_loader, optimizer, criterion, device, epochs=5)\n","    \n","    model_save_path = f\"trained_model_{os.path.basename(corpus_path)}_{model_type}.pt\"\n","    torch.save({'model_state_dict': model.state_dict(), 'vocab': vocab}, model_save_path)\n","    print(f\"Model saved to {model_save_path}\")\n","    \n","    # Calculate perplexity for each set\n","    train_file = f\"{os.path.basename(corpus_path)}_train_perplexity_{model_type}.txt\"\n","    val_file = f\"{os.path.basename(corpus_path)}_val_perplexity_{model_type}.txt\"\n","    test_file = f\"{os.path.basename(corpus_path)}_test_perplexity_{model_type}.txt\"\n","    \n","    print(\"\\nCalculating and saving Train Perplexity...\")\n","    set_perplexity(model, train_loader, device, train_file)\n","    print(\"\\nCalculating and saving Validation Perplexity...\")\n","    set_perplexity(model, val_loader, device, val_file)\n","    print(\"\\nCalculating and saving Test Perplexity...\")\n","    set_perplexity(model, test_loader, device, test_file)\n","    print(f\"Perplexity scores saved in: {train_file}, {val_file}, {test_file}\")\n","    \n","    return model_save_path\n","\n","\n","\n","# ---------------------------\n","# Main\n","# ---------------------------\n","if __name__ == \"__main__\":\n","    pride_corpus = \"/kaggle/input/corpus/Pride_and_Prejudice.txt\"\n","    ulysses_corpus = \"/kaggle/input/corpus/Ulysses.txt\"\n","    \n","    saved_models = []\n","    for corpus in [pride_corpus,ulysses_corpus]:\n","        saved_models.append(run_experiment(corpus, model_type=\"LSTM\"))\n","\n","    print(\"\\nSaved models:\", saved_models)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2025-02-13T12:58:40.504735Z","iopub.status.busy":"2025-02-13T12:58:40.504361Z","iopub.status.idle":"2025-02-13T13:03:15.025765Z","shell.execute_reply":"2025-02-13T13:03:15.024819Z","shell.execute_reply.started":"2025-02-13T12:58:40.504708Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-17-99587ce5ebd8>:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(model_path, map_location=device)\n"]},{"name":"stdout","output_type":"stream","text":["LSTM Next-Word Generator\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  He went to\n"]},{"name":"stdout","output_type":"stream","text":["[5959, 2825, 4799]\n","\n","Top predictions:\n","  the: 0.0921\n","  bed: 0.0904\n","  see: 0.0435\n","  be: 0.0398\n","  her: 0.0391\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  He went to the\n"]},{"name":"stdout","output_type":"stream","text":["[5959, 2825, 4799, 4764]\n","\n","Top predictions:\n","  library: 0.1075\n","  rest: 0.0621\n","  house: 0.0432\n","  girls: 0.0366\n","  family: 0.0216\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  She is a very good\n"]},{"name":"stdout","output_type":"stream","text":["[5959, 1244, 4965, 4528, 1210]\n","\n","Top predictions:\n","  notion: 0.3046\n","  match: 0.1051\n","  luck: 0.0692\n","  man: 0.0166\n","  sort: 0.0155\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  She is a good\n"]},{"name":"stdout","output_type":"stream","text":["[5959, 1244, 4965, 1210]\n","\n","Top predictions:\n","  luck: 0.1773\n","  deal: 0.1761\n","  match: 0.0413\n","  cook: 0.0368\n","  journey: 0.0327\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  He is a good\n"]},{"name":"stdout","output_type":"stream","text":["[5959, 1244, 4965, 1210]\n","\n","Top predictions:\n","  luck: 0.1773\n","  deal: 0.1761\n","  match: 0.0413\n","  cook: 0.0368\n","  journey: 0.0327\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  He is going to\n"]},{"name":"stdout","output_type":"stream","text":["[5959, 1244, 1176, 4799]\n","\n","Top predictions:\n","  be: 0.3092\n","  the: 0.0398\n","  gretna: 0.0278\n","  a: 0.0265\n","  find: 0.0195\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  He is the\n"]},{"name":"stdout","output_type":"stream","text":["[5959, 1244, 4764]\n","\n","Top predictions:\n","  most: 0.1124\n","  greatest: 0.0956\n","  son: 0.0709\n","  case: 0.0547\n","  matter: 0.0216\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  She is the\n"]},{"name":"stdout","output_type":"stream","text":["[5959, 1244, 4764]\n","\n","Top predictions:\n","  most: 0.1124\n","  greatest: 0.0956\n","  son: 0.0709\n","  case: 0.0547\n","  matter: 0.0216\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  He is going to be a\n"]},{"name":"stdout","output_type":"stream","text":["[5959, 1244, 1176, 4799, 2019, 4965]\n","\n","Top predictions:\n","  most: 0.1072\n","  little: 0.0591\n","  great: 0.0514\n","  man: 0.0497\n","  very: 0.0415\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  He is going to be a very\n"]},{"name":"stdout","output_type":"stream","text":["[5959, 1244, 1176, 4799, 2019, 4965, 4528]\n","\n","Top predictions:\n","  agreeable: 0.1968\n","  different: 0.1271\n","  great: 0.0355\n","  handsome: 0.0259\n","  sensible: 0.0256\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  He is going to be a big\n"]},{"name":"stdout","output_type":"stream","text":["[5959, 1244, 1176, 4799, 2019, 4965, 5959]\n","\n","Top predictions:\n","  </s>: 0.1149\n","  and: 0.1047\n","  woman: 0.0713\n","  man: 0.0577\n","  agreeable: 0.0339\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  I am going to\n"]},{"name":"stdout","output_type":"stream","text":["[5959, 1997, 1176, 4799]\n","\n","Top predictions:\n","  be: 0.2162\n","  the: 0.1241\n","  gretna: 0.0380\n","  a: 0.0352\n","  have: 0.0315\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  I am feeling\n"]},{"name":"stdout","output_type":"stream","text":["[5959, 1997, 2323]\n","\n","Top predictions:\n","  that: 0.1606\n","  </s>: 0.1460\n","  a: 0.0753\n","  the: 0.0521\n","  in: 0.0495\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  I feel sad\n"]},{"name":"stdout","output_type":"stream","text":["[5959, 3035, 2748]\n","\n","Top predictions:\n","  business: 0.1187\n","  omen: 0.0993\n","  premises: 0.0147\n","  setting: 0.0058\n","  depended: 0.0058\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  I a feeling very\n"]},{"name":"stdout","output_type":"stream","text":["[5959, 4965, 2323, 4528]\n","\n","Top predictions:\n","  large: 0.0392\n","  different: 0.0385\n","  pleasing: 0.0299\n","  great: 0.0288\n","  woman: 0.0284\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  It is difficult for me to\n"]},{"name":"stdout","output_type":"stream","text":["[5959, 1244, 4949, 433, 5146, 4799]\n","\n","Top predictions:\n","  follow: 0.0456\n","  do: 0.0411\n","  accept: 0.0345\n","  pass: 0.0300\n","  pay: 0.0263\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  he is the\n"]},{"name":"stdout","output_type":"stream","text":["[472, 1244, 4764]\n","\n","Top predictions:\n","  most: 0.1286\n","  case: 0.1101\n","  greatest: 0.0921\n","  son: 0.0735\n","  matter: 0.0299\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  he is the son\n"]},{"name":"stdout","output_type":"stream","text":["[472, 1244, 4764, 1615]\n","\n","Top predictions:\n","  of: 0.7905\n","  </s>: 0.0915\n","  in: 0.0145\n","  he: 0.0120\n","  and: 0.0102\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  he is the handsomest young\n"]},{"name":"stdout","output_type":"stream","text":["[472, 1244, 4764, 2491, 3700]\n","\n","Top predictions:\n","  man: 0.8379\n","  lady: 0.0944\n","  woman: 0.0121\n","  men: 0.0108\n","  ladies: 0.0077\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  Her carriage remained at the\n"]},{"name":"stdout","output_type":"stream","text":["[5959, 2067, 4431, 455, 4764]\n","\n","Top predictions:\n","  door: 0.2386\n","  same: 0.0964\n","  instrument: 0.0344\n","  parsonage: 0.0339\n","  inn: 0.0255\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  Her carriage\n"]},{"name":"stdout","output_type":"stream","text":["[5959, 2067]\n","\n","Top predictions:\n","  and: 0.1070\n","  </s>: 0.0604\n","  by: 0.0434\n","  the: 0.0423\n","  drove: 0.0366\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  You have no\n"]},{"name":"stdout","output_type":"stream","text":["[5959, 1112, 1820]\n","\n","Top predictions:\n","  objection: 0.0638\n","  charms: 0.0407\n","  reason: 0.0382\n","  more: 0.0301\n","  compassion: 0.0269\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  Can you possibly\n"]},{"name":"stdout","output_type":"stream","text":["[5959, 3205, 1809]\n","\n","Top predictions:\n","  guess: 0.0403\n","  and: 0.0348\n","  </s>: 0.0331\n","  get: 0.0207\n","  wonder: 0.0168\n"]},{"name":"stdout","output_type":"stream","text":["\n","Enter input sentence (or type 'exit' to quit):  exit\n"]}],"source":["import torch\n","import torch.nn as nn\n","import sys\n","import os\n","import numpy as np\n","\n","\n","# ---------------------------\n","# Function to Load the Pre-trained LSTM Model\n","# ---------------------------\n","def load_model(model_path, device):\n","    \"\"\"\n","    Loads a pre-trained LSTM language model from a checkpoint.\n","    \n","    Args:\n","      model_path (str): Path to the trained model file.\n","      device (torch.device): Device to load the model on.\n","      \n","    Returns:\n","      model (torch.nn.Module): Loaded LSTM model.\n","      vocab (dict): Vocabulary used in training.\n","    \"\"\"\n","    checkpoint = torch.load(model_path, map_location=device)\n","    vocab = checkpoint['vocab']\n","    vocab_size = len(vocab)\n","    embed_size = 300   # Must match training configuration\n","    hidden_size = 256   # Must match training configuration\n","    num_layers = 1      # As used during training\n","    dropout_rate = 0.3  # As used during training\n","\n","    model = LSTMLanguageModel(vocab_size, embed_size, hidden_size, num_layers, dropout_rate)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    model.to(device)\n","    model.eval()\n","    return model, vocab\n","\n","# ---------------------------\n","# Next-Word Prediction Function for LSTM\n","# ---------------------------\n","def predict_next_words(model, vocab, input_text, k, device):\n","    \"\"\"\n","    Given an input sentence, predicts the top k most probable next words.\n","    \n","    The function:\n","      1. Tokenizes the input sentence (whitespace splitting).\n","      2. Converts words to indices.\n","      3. Creates an input tensor of shape (1, seq_length).\n","      4. Passes the tensor through the model.\n","      5. Reshapes the model output to (1, seq_length, vocab_size).\n","      6. Extracts the probabilities for the last time step.\n","      7. Returns the top k words sorted by probability.\n","    \n","    Args:\n","      model (torch.nn.Module): The trained LSTM model.\n","      vocab (dict): Vocabulary mapping words to indices.\n","      input_text (str): The input sentence.\n","      k (int): Number of top predictions to return.\n","      device (torch.device): Device for inference.\n","      \n","    Returns:\n","      List[Tuple[str, float]]: List of tuples containing (word, probability).\n","    \"\"\"\n","    # Tokenize input sentence\n","    words = input_text.strip().split()\n","    if not words:\n","        print(\"[ERROR] Input sentence cannot be empty.\")\n","        return []\n","    \n","    # Convert words to indices; use <UNK> if not found\n","    context_indices = [vocab.get(word, vocab[\"<UNK>\"]) for word in words]\n","    # Create input tensor (batch_size=1)\n","    context_tensor = torch.tensor(context_indices, dtype=torch.long, device=device).unsqueeze(0)\n","    print(context_indices)\n","    with torch.no_grad():\n","        log_probs, _ = model(context_tensor)\n","    \n","    # Our model returns log_probs in flattened form: shape = (batch_size * seq_length, vocab_size)\n","    # We need to reshape to get predictions for the final time step.\n","    batch_size = context_tensor.size(0)\n","    seq_len = context_tensor.size(1)\n","    vocab_size = len(vocab)\n","    log_probs = log_probs.view(batch_size, seq_len, vocab_size)\n","    \n","    # Use the final time step (last token in the input sequence) for prediction.\n","    last_log_probs = log_probs[:, -1, :]  # Shape: (1, vocab_size)\n","    probabilities = torch.exp(last_log_probs).squeeze(0)  # Shape: (vocab_size,)\n","    \n","    # Get top k indices sorted in descending order.\n","    topk = torch.topk(probabilities, k, largest=True, sorted=True)\n","    topk_indices = topk.indices.cpu().numpy().flatten()\n","    topk_probs = topk.values.cpu().numpy().flatten()\n","    \n","    # Create an inverse mapping: index -> word\n","    index_to_word = {idx: word for word, idx in vocab.items()}\n","    \n","    predictions = [(index_to_word.get(int(idx), \"<UNK>\"), float(prob)) for idx, prob in zip(topk_indices, topk_probs)]\n","    return predictions\n","\n","# ---------------------------\n","# Main Script for Generation\n","# ---------------------------\n","if __name__ == \"__main__\":\n","    # Hardcoded parameters:\n","    # Using LSTM model, corpus path is \"Pride_and_Prejudice.txt\", and k = 5.\n","    model_path = \"trained_model_Pride_and_Prejudice.txt_LSTM.pt\"  # Ensure this checkpoint exists.\n","    corpus_path = \"/kaggle/input/corpus/Pride_and_Prejudice.txt\"  # Hardcoded corpus path.\n","    k = 5\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    \n","    # Load the pre-trained LSTM model and vocabulary.\n","    model, vocab = load_model(model_path, device)\n","    \n","    print(\"LSTM Next-Word Generator\")\n","    while True:\n","        input_text = input(\"\\nEnter input sentence (or type 'exit' to quit): \").strip()\n","        if input_text.lower() == \"exit\":\n","            break\n","        \n","        predictions = predict_next_words(model, vocab, input_text, k, device)\n","        \n","        if predictions:\n","            print(\"\\nTop predictions:\")\n","            for word, prob in predictions:\n","                print(f\"  {word}: {prob:.4f}\")\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2025-02-17T06:40:49.536060Z","iopub.status.busy":"2025-02-17T06:40:49.535681Z","iopub.status.idle":"2025-02-17T06:40:58.491356Z","shell.execute_reply":"2025-02-17T06:40:58.490454Z","shell.execute_reply.started":"2025-02-17T06:40:49.536037Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["updating: kaggle/working/ (stored 0%)\n","updating: kaggle/working/trained_model_5_Pride_and_Prejudice.txt.pt (deflated 8%)\n","updating: kaggle/working/3_Ulysses.txt_val_perplexity.txt (deflated 58%)\n","updating: kaggle/working/trained_model_5_Ulysses.txt.pt (deflated 7%)\n","updating: kaggle/working/3_Pride_and_Prejudice.txt_test_perplexity.txt (deflated 61%)\n","updating: kaggle/working/.virtual_documents/ (stored 0%)\n","updating: kaggle/working/trained_model_3_Pride_and_Prejudice.txt.pt (deflated 8%)\n","updating: kaggle/working/5_Ulysses.txt_train_perplexity.txt (deflated 61%)\n","updating: kaggle/working/trained_model_3_Ulysses.txt.pt (deflated 8%)\n","updating: kaggle/working/3_Ulysses.txt_train_perplexity.txt (deflated 58%)\n","updating: kaggle/working/3_Pride_and_Prejudice.txt_train_perplexity.txt (deflated 63%)\n","updating: kaggle/working/3_Pride_and_Prejudice.txt_val_perplexity.txt (deflated 61%)\n","updating: kaggle/working/5_Pride_and_Prejudice.txt_test_perplexity.txt (deflated 63%)\n","updating: kaggle/working/5_Ulysses.txt_test_perplexity.txt (deflated 60%)\n","updating: kaggle/working/3_Ulysses.txt_test_perplexity.txt (deflated 57%)\n","updating: kaggle/working/5_Pride_and_Prejudice.txt_train_perplexity.txt (deflated 64%)\n","updating: kaggle/working/5_Ulysses.txt_val_perplexity.txt (deflated 61%)\n","updating: kaggle/working/5_Pride_and_Prejudice.txt_val_perplexity.txt (deflated 63%)\n"]}],"source":["!zip -r my_files.zip /kaggle/working/"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2025-02-17T07:09:43.507284Z","iopub.status.busy":"2025-02-17T07:09:43.506982Z","iopub.status.idle":"2025-02-17T07:09:44.189756Z","shell.execute_reply":"2025-02-17T07:09:44.188704Z","shell.execute_reply.started":"2025-02-17T07:09:43.507263Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'/kaggle/working/model_backup.zip'"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# List files in the working directory to confirm\n","import shutil\n","\n","# Compress the .pt file\n","shutil.make_archive(\"/kaggle/working/model_backup\", 'zip', \"/kaggle/working\", \"trained_model_3_Pride_and_Prejudice.txt.pt\")\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":6594718,"sourceId":10650231,"sourceType":"datasetVersion"}],"dockerImageVersionId":30840,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
